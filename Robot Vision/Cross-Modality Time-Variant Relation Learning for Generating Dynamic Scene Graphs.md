# Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene Graphs

## 背景

从视频片段中生成动态场景图片可以在一系列有挑战的任务如环境感知、自动导航或者自动驾驶车辆和移动机器人的路径规划中提升语义视觉理解。但是，我们很难从一系列帧中通过动态场景找到时间变化的关系。

场景图片中实体和关系通常用三元组`<主体-动词-客体`。其中的挑战是：一些微小的变化很可能导致关系的转变，而我们很难识别这种变化。也有些时候即使动作再怎么改变，但是之间的关系依旧没有变化。现存的方法效果都不是很好。

本文提出一种$\text{TR}^2$的方法，在动态场景图片中对时序变化进行建模。具体而言，就是将一些表示关系的embedding词嵌入进去。用带特征提取模块的Transformer和附加的消息token来描述相邻帧之间的关系，比如“一张`主体``动词``客体`的图片”。除此之外$\text{TR}^2$使用一种消息token，通过考虑当前帧对最后一帧做出的改变程度来强调关系的改变。

最后，$\text{TR}^2$的表现超过了目前最好的方法。
