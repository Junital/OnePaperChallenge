# Cross-Modality Time-Variant Relation Learning for Generating Dynamic Scene Graphs

## 背景

从视频片段中生成动态场景图片可以在一系列有挑战的任务如环境感知、自动导航或者自动驾驶车辆和移动机器人的路径规划中提升语义视觉理解。但是，我们很难从一系列帧中通过动态场景找到时间变化的关系。

场景图片中实体和关系通常用三元组`<主体-动词-客体>`。其中的挑战是：一些微小的变化很可能导致关系的转变，而我们很难识别这种变化。也有些时候即使动作再怎么改变，但是之间的关系依旧没有变化。现存的方法效果都不是很好。

本文模型收到知识提取的启发，知识提取原本被提出用来从一个又大又深的神经网络提取成一个小的网络。基于特征的知识提取用隐藏层的输出，即特征映射作为知识对小网络的训练进行监督。

## $\text{TR}^2$

本文提出一种$\text{TR}^2$的方法，在动态场景图片中对时序变化进行建模。具体而言，就是将一些表示关系的embedding词嵌入进去。用带特征提取模块的Transformer和附加的消息token来描述相邻帧之间的关系，比如“一张`主体``动词``客体`的图片”。除此之外$\text{TR}^2$使用一种消息token，通过考虑当前帧对最后一帧做出的改变程度来强调关系的改变。

令$G = \{G_t\}_{t=1}^T$表示一个视频片段中的动态场景图片，其中$t$代表帧的索引，$T$代表视频中所有打上标签的帧的总数。假设$\{i_t\}^T_{t=1}$为视频中的帧，则$G_t = \{V_t,E_t\}$代表帧$i_t$的场景图。其中，$V_t$代表实体集合，作为结点；$E_t$代表关系集合，作为$V_i$中结点之间的边。

### 大致框架

首先，帧作为输入进入一个物体检测模型，输出实体、检测框、类别和物体的视觉特征。视觉特征被当作实体关系的一部分。并且，本文讲主体和客体的检测框进行合并。然后，对图片进行裁剪，用一个Encoder对图片进行引导。对于关系表示，本文将其作为关系特征融合模块的输入，输出关系的类别、和实体形成场景图。

![Fig1](./Fig/TR2%20framework.png)

### 关系特征融合

首先，对关系表示进行一个帧内空间特征融合。然后按照实体和时间顺序对关系进行重新排列。之后，用时间Decoder进行帧外的特征融合。就此通过短语token间接得到了时间变化的关系。最后，在此按照帧的关系进行重新排列。

### 跨模态特征引导



## 检验和测试

最后，$\text{TR}^2$的表现超过了目前最好的方法。
