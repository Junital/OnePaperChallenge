# ViT-Lens: Towards Omni-modal Representations

## 背景

为了提升AI agent的能力，大基础模型能提升推理能力和指令执行能力。但是目前的方法仅关注于视觉和语言模态，忽略了真实世界环境中潜在的多种模态信息。然而，目前以数据为驱动的视觉语言模型如果要加入其他模态开销会很大，甚至不可行。本文提出ViT-Lens，通过利用预训练ViT并将新模态对齐到预定义的空间中，从而进行高效全模态表征学习。具体来说，特定模态透镜将模态信号投射到一个中间态嵌入空间中，之后通过预训练过视觉知识的ViT来进行处理。编码后的表征被优化至与基础模型定义好的独立模态空间进行对齐。ViT-Lens为增加模态的表征学习提供了一个统一解决思路，有两个优点：

1. 不需要太多参数和数据体量。
2. 通过模态对齐实现许多涌现的下游能力。
