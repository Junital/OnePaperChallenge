# ChatEval: Towards Better LLM-based Evaluators through Multi-Agent Debate

## 背景

实验的结果建议单智能体方法需要在目前有效性和人类评估质量的差距上继续进行推进。由于真实世界中人类评估过程需要多个人给出意见，本文提出一个多智能体争论框架。本文构建了一个多智能体裁判队伍（ChatEval）来自动讨论并评估自然语言生成模型生成响应的质量。
