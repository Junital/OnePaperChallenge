# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## 背景

语言模型预训练在许多自然语言处理任务中能有效提高表现，包括句子级别的任务（比如自然语言推理、预测句子之间的关系）和token级别的任务（比如命名实体识别、问答）。

目前，有两种策略将预训练语言表示用于下游任务：基于特征的方法和微调方法。基于特征的方法将预训练表示当作附加的特征，加入到特定任务模型中。微调方法引入了最小任务特定参数，并在训练中简单微调所有参数。这两种方法都使用无方向的目标函数来进行预训练。

对于问答这种任务，我们更需要从双向找到上下文。

## BERT

整体分为两个阶段：预训练和微调阶段。
