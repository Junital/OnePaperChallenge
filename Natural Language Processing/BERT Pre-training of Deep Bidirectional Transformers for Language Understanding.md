# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## 背景

语言模型预训练在许多自然语言处理任务中能有效提高表现，包括句子级别的任务（比如自然语言推理、预测句子之间的关系）和token级别的任务（比如命名实体识别、问答）

## BERT
