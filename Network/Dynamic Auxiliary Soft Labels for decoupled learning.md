# Dynamic Auxiliary Soft Labels for decoupled learning

## 背景

数据集分布不均是深度学习中最主要的挑战之一。CNN在小样本的情况下表现较差。

长尾分布：只有少部分类别占据着大多数数据，其他类别占据得很少。

下面是一些主流的解决长尾分布的方法：

- **重采样方法**：主要就是将尾类别过采样，将头类别下采样两种方法。
- **基于代价灵敏的方法**：计算损失函数的时候，将更多的损失倾向于尾类别。
- **分隔学习**，一种将特征学习阶段和分类器学习阶段分开学习的方法，能有效提升网络的表现能力。
- **基于迁移学习的方法**：将从头类别学到的知识迁移到学习尾类别中。
- **软标签方法**：通过生成软标签进行训练，来提高模型的泛化能力。

Label Smoothing方法效果反而恶化，原因可能是在生成软标签的时候，偏差会倾向于大比例标签。Online Label Smoothing方法，在特征学习阶段中的预测是存在偏差的，但是分类器学习阶段中的表示是固定不变的。

## DaSL

研究显示，如果特征学习阶段和分类器学习阶段分离开，那么模型准确率会得到很大的提升。硬标签中，类别概率非0即1，只有正确的类别概率才为1。而软标签的概率如下所示：

$$
\begin{aligned}
q_i^j = \begin{cases}
1 - \varepsilon & \text{if}\ j=k\\
\varepsilon / (K - 1) & \text{otherwise}.
\end{cases}
\end{aligned}
$$

通过研究显示，Label Smoothing方法对于结果有消极的影响，可能的原因是其使用了统一的分布，而这种同一分布加强了模型的偏置。因此，本文考虑一种不统一的标签软化方法。

Online Label Smoothing方法考虑了不同类别的真实关系，是一种非统一的软标签生成方法。其定义了一个软标签矩阵$S \in \mathbb{R}^{K \times K}$，$S = \left \{ S_1, S_2, \dots, S_K\right \}$。Online Label Smoothing方法根据模型在上一个epoch中的表现，生成此epoch的软标签，公式如下：

$$S^t_{y_i, k} = S^t_{y_i, k} + M_{\text{last}}(k | x_i)$$

根据实验显示，Online Label Smoothing的效果也不尽人意，主要是因为在第二阶段特征表示被冻结了，而特征表示对软标签的生成有很大的影响。因此，本文认为不能将第一阶段的特征表示用于第二阶段的软标签生成。

本文设计一个专用的辅助网络AN来为两个学习阶段生成软标签。根据研究显示，自我提取可以使用模型的预测来找到类别之间的联系。AN整体表现就像一个指导网络。在特征学习阶段，软标签可以让网络学习到类之间的差异。在分类器学习阶段，软标签可以减轻模型预测的过度自信。

在**特征学习阶段**，本文使用一个实例采样的方法同时训练特征提取网络、分类器和辅助网络。辅助网络使用中间层特征映射来为特征提取网络提供特征层面提取。同时，辅助网络通过生成软标签为分类器提供标签层面辅助监督。通过这两个方面的信息进行监督可以让特征提取网络学习到一个更好的大致特征表示。

具体而言，对于输入$(x_i, y_i)$，通过特征提取得到每一层的特征$\left [F_i^1, F_i^2, \dots, F_i^B\right ]$并分类得到分布$s_c = (s_c^1, s_c^2, \dots, s_c^K)$，用交叉熵和one-hot硬标签计算损失函数，从而进行监督：

$$ L_\text{gt2c}(y_i, s_c) = -\sum_{j=1}^K y_i^j \log s_c^j$$

之后，辅助网络将每一层的特征$\left [F_i^1, F_i^2, \dots, F_i^B\right ]$作为输入，得到特征$\left [FA_i^1, FA_i^2, \dots, FA_i^B\right ]$和预测分布$s_a = (s_a^1, s_a^2, \dots, s_a^K)$，同样也是用交叉熵和one-hot硬编码计算损失函数：

$$ L_\text{gt2a}(y_i, s_a) = -\sum_{j=1}^K y_i^j \log s_a^j$$

为了通过使用软标签，提升特征表示质量，本文使用辅助网络来制作软标签。本文借鉴了知识提取中的损失函数，设置温度系数$T$，将$s_a, s_c$转变为$\tilde{s}_a, \tilde{s}_c$，公式为$\tilde{s}^i = \frac{\exp(s^i / T)}{\sum_j \exp(s^j / T)}$，损失函数设置如下：

$$ L_\text{a2c}(s_a, s_c) = \sum_{j=1}^K\tilde{s^j_a} \log \frac{\tilde{s^j_a}}{\tilde{s^j_c}}$$

![Fig1](./fig/DaSL.png)

同时，本文在特征学习中引入一个特征层面提取方法，并通过多尺度特征融合提升大致特征。
