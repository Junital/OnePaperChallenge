# BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding

## 背景

语言模型预训练在许多自然语言处理任务中能有效提高表现，包括句子级别的任务（比如自然语言推理、预测句子之间的关系）和token级别的任务（比如命名实体识别、问答）。

目前，有两种策略将预训练语言表示用于下游任务：基于特征的方法和微调方法。基于特征的方法将预训练表示当作附加的特征，加入到特定任务模型中。微调方法引入了最小任务特定参数，并在训练中简单微调所有参数。这两种方法都使用无方向的目标函数来进行预训练。

对于问答这种任务，我们更需要从双向找到上下文。

## BERT

整体分为两个阶段：预训练和微调阶段。在预训练阶段，模型用未标注的数据进行训练。在微调阶段，首先模型的参数初始化为预训练结束时的参数，并用标注的数据进行参数微调。不同的任务最后微调得到的参数可能不一样。

BERT的模型结构是一个多层双向Transformer编码器。其中包括$L$个Transformer模块，$H$个隐藏层和$A$个自注意力头。
