# Neural-Symbolic Recursive Machine for Systematic Generalization

## 背景

目前学习模型经常难以解决人类系统泛化问题，特别是从限制的数据中学到组合规则并外推为新的组合。

系统组构性是人类智力的一个起决定性作用的特征，表示从有限的、已知的部分中生成无限的演绎的代数能力。通过评估，传统的神经网络通常在此方面表现很差，导致了目前对归纳偏置的探索，以寻求更好的泛化能力。

当然也有一些创新：相关性位置编码和层级权重共享被使用来提升Transformer模型的泛化能力。除此之外，神经符号堆叠机在相关数据集上取得很高的准确率，大语言模型也被引领向组构性语义分析任务。但是，这些任务都需要特定领域的知识，并且很难进行领域转移。

在系统泛化的优化表示上，连接主义和符号主义一直保持着争论。连接主义认为将多个神经元表示一个概念的**分布式表示**更好，符号主义认为每个符号表示一个基本概念、复杂的概念就由多个符号组合进行表示的**物理符号系统**更好。

## NSR

为了追求在多种领域中取得人类系统泛化能力，本文引入神经符号回归机（NSR）。其核心为基本符号系统（GSS），可以直接从训练集中处理组合型的语法和语义，不需要特定领域的知识。

![Fig1](./fig/GSS%20examples.png)

NSR采用一种模块化的设计，将神经感知、语法分析、语义推理等模块结合起来。这些模块会通过一种新的演绎-外展推理算法被同时训练（joint learning）。

初始化，神经网络充当着感知模块，将基本符号作为原始输入。通过基于转移的神经依赖分析器，这些符号被组织成一棵语法树。

由于内部GSS标注的缺少和模型不可分的组成部分，NSR的端到端优化面临挑战。为了尝试绕过这些障碍，本文设计了一个基于概率的学习框架和一个新颖的面向joint learning的演绎-外展推理算法。这个算法首先通过贪心演绎生成一个初始的GSS。然后进行自上而下、基于搜索的外展，通过探索邻接点寻找更优解以至获得正确答案来优化初始的GSS。优化后的GSS利用伪监督，让NSR每个部分进行单独训练。

### GSS

由于符号系统的可解释性强，它会相比于分布式表示有更强的抽象能力和泛化能力。但是，构建符号系统可能会很脆弱，受到符号奠基问题的攻击。

因此，本文引入了基本符号系统作为系统泛化的内部表示，将感知、语法、语义进行无缝衔接。具体来说，GSS是一棵有向树$T=<(x, s, v), e>$。每个结点是一个三元组：基本输入$x$、抽象符号$s$和语义含义$v$。边$e$代表着语义依赖关系，比如$i\rightarrow j$就表示$i$的含义依赖于$j$。

### 训练过程

NSR一共有三个训练模块：对符号奠基的神经感知、推断符号之间依赖关系的依赖分析和演绎语义的编码生成。在训练时没有标准真实的GSS情况下，这些模块必须进行无中途监督的端到端学习。

![Fig2](./fig/NSR%20training%20pipeline.png)

#### 神经感知

此模块将一个原始的输入$x$转变为一个符号序列$s$，用序号列表表示。这个模块主要用于解决原始输入信号的感知易变性，保证每个$w_i \in s$和特定输入段$x_i \in x$相匹配。具体这种关系如下所示：

$$p(s|x;\theta_p) = \prod_i p(w_i|x_i; \theta_p) = \prod_i \text{softmax}(\phi(w_i, x_i; \theta_p))$$

其中，$\phi(w_i, x_i; \theta_p)$表示一个用参数$\theta_p$组成的神经网络制作的打分函数。这种神经网络可以被预训练，比如用卷积神经网络处理图片输入。

#### 依赖分析

为了演绎符号之间的依赖关系，本文使用了一个广泛应用于自然语言语句分析的基于转移的神经依赖分析器。就像状态机一样，分析器识别可能的转移，将输入的序列变为依赖树，不断迭代指导分析过程变得完整。(很妙啊，就像编译技术里的一样)一个两层的前馈神经网络在给定状态表示后，决定下一步的转移。

![Fig3](./fig/Transition-based%20Dependency%20Parser.png)

具体来说，给定一个输入序列$s$，分析过程如下定义：

$$p(e|s;\theta_s) = p(\tau|s; \theta_s) = \prod_{t_i\in\tau} p(t_i|c_i;\theta_s)$$

其中，$\theta_s$表示分析器的参数，$\tau = \{t_1,t_2,\dots,t_l\}\models e$是用来生成依赖树$e$的一些列转移，$c_i$是第$i$步的状态表示。

#### 编码归纳

受到了编码归纳发展的影响，本文用函数氏的编码表达符号的语义，将学习表达为一个编码归纳的过程。符号化的编码会提供增强的泛化性和可解释性，并且凭借纯分析方法提高效率。具体来说，给定输入符号$s$和依赖$e$，关系被定义为：

$$p(v|e,s;\theta_l) = \prod_i p(v_i|s_i, \text{children}(s_i);\theta_l)$$

其中，$\theta_l$代表每个符号对应的归纳编码集。实际使用的是符号编码，保证推理过程是唯一确定的。

#### 模型推理

比如HINT任务，通过识别输入，感知到一组符号$2+3\times 9$，分析阶段将其转化为一棵语法树，之后进行计算得到结果$29$。

### 学习算法

NSR中的GSS是隐藏且不可分割的，这使得直接使用反向传播变得不可能。传统的方法比如强化学习会面临缓慢且不持续的收敛的挑战。本文需要一种更加有效的学习算法。具体来说，给定输入$x$、中间体GSS $T=<(x, s, v), e>$和输出$y$，脱离于$T$的观测似然$(x, y)$为：

$$p(y|x;\Theta) = \sum_Tp(T,y|x;\Theta)=\sum_{s,e,v}p(s|x;\theta_p)p(s|x;\theta_s)p(s|x;\theta_l)p(y|v)$$

从最大化似然估计的视角来看，NSR学习的目标就是最大化观测数据$\log$似然$L(x, y) = \log p(y|x)$。$L$对$\theta_p,\theta_s,\theta_l$的梯度如下所示：

$$
\begin{aligned}
\nabla_{\theta_p} L(x, y) &= \mathbb{E}_{T\sim p(T|x, y)}\left [\nabla_{\theta_p}\log p(s|x;\theta_p)\right ]\\
\nabla_{\theta_s} L(x, y) &= \mathbb{E}_{T\sim p(T|x, y)}\left [\nabla_{\theta_s}\log p(s|x;\theta_s)\right ]\\
\nabla_{\theta_l} L(x, y) &= \mathbb{E}_{T\sim p(T|x, y)}\left [\nabla_{\theta_l}\log p(s|x;\theta_l)\right ]
\end{aligned}
$$

其中，$p(T|x, y)$代表给定$(x, y)$下$T$的厚颜分布，可以如下表示：

$$p(T|x,y) = \frac{p(T,y|x;\Theta}{\sum_{T'}p(T',y|x;\Theta)} = \left\{\begin{matrix}
 0, & \text{if\ \ } T\notin Q\\
 \frac{p(T|x;\Theta)}{\sum_{T'\in Q}p(T'|x;\Theta)}, & \text{if\ \ } T\in Q
\end{matrix}\right. $$

其中$Q$是$T$和$y$的交集。

由于对于此后验分布求均值存在计算上的挑战，本文使用蒙特卡洛采样来求近似解。这种优化策略会从后验分布中采样一个解决方案，并通过监督训练迭代更新每个模块，避免了从一个大范围、稀疏分布空间中进行采样的困难性。

#### 演绎-外展

![Fig4](./fig/abduction%20search.png)

通过演绎-外展算法，本文提出的学习方法可以有效地从后验概率中进行采样。对于一个实例$(x, y)$，本文初始化运用贪心演绎从$x$中得到一个初始化的GSS $T=<(x, \hat{s}, \hat{v}), \hat{e}>$。为了在训练中将$T^*$与正确答案$y$对齐，本文使用一种自上而下的外展搜索，在$T$的邻居之间进行迭代，对感知、语法、语义进行调整。一旦找到一个$T^*$和生成的$y$一致，或者已经达到了预设步数，那么搜索就停止了。理论上，这种方法表现为对$p(T|x, y)$进行梅特罗波利斯-黑斯廷斯采样，是模型训练的有效方法。

![Fig5](./fig/deduction-abduction%20process.png)

然后通过数学理论计算，证明NSR具备了表达能力和系统泛化能力。

## 结果

本文发现，NSR的基于“等变”和“组合”这两个归纳基础的设计，使得其能熟练地掌握不同的seq-to-seq任务，并取得空前的系统泛化。

本文通过四个benchmark进行效能评估，分别为语义分析任务SCAN、字符串操作任务PCFG、算数推理HINT和一个组合性机器翻译任务。经过与目前的模型比对，NSR在泛化和迁移中取得优势。准确率分别为100%、100%、提升23%。

## 一些限制

- 如果存在噪声和充足的概念，会增加GSS的空间，影响训练的效率。
- 决定论的代码生成对NSR应用于真实场景会有限制，比如一个句子会有多种翻译的版本。

## 个人感想

1. 本文的数学证明的地方我可以学习学习，感觉很严谨。
2. 依赖分析和代码归纳方面的创新性很强，值得学习，特别是将神经网络应用到一些计算中。
3. 分析能力感觉很不错，但是纠错能力好像比较差。
