# Post Tuned Hashing: A New Approach to Indexing High-dimensional Data

## 背景

传统基于树的索引方法会遭受维度灾难（维度越高、邻居越少）。而哈希是一种将高维数据映射到保留相似性的二进制编码中的有效方法。通过二进制编码，最近邻搜索任务可以有效通过比特级操作完成。

二进制哈希技术可以大致分为监督（半监督）方法和无监督方法。监督方法将利用标签信息学习二进制编码，使其专门适用于语义搜索；无监督哈希方法通过探索数据的复杂特性学习二进制编码，可被有效用于无标签的数据。本文将关注于无监督哈希。

目前现存的方法遵从一个两阶段范式：在投射阶段会将原始数据映射到低纬度连续空间中；在二进制化阶段会将投射结果二进制化为最终的二进制编码。之前的研究主要关注于这两个阶段的改进与优化，尝试尽可能保留更多的邻居关系。大多数方法目前都想在投射阶段通过探索原始数据的不同属性（比如分布、配对关系、多种结构等）来保留相似性。

尽管有各种尝试，但二进制化阶段不可避免会破坏原始数据的相邻结构，导致巨大的**邻居错误**（原始数据空间中的邻居点索引得到非邻居点、非邻居点索引得到邻居点）。因此，这些方法会出现巨大相似性损失和不尽人意的索引表现。

![Fig1](./fig/neighbor%20error.png)

## PTH方法

### 无监督二进制哈希

给定一个集合$X \in \mathbb{R}^{d\times n}$，其中包括$n$个数据点$\{x_i\}_{i=1}^n \in \mathbb{R}^d$。无监督二进制哈希会首先通过线性或者非线性方程$P:\mathbb{R}^d \rightarrow \mathbb{R}^m$将数据点投射到一个低纬度的连续流形（可理解为空间）。之后将投射结果二进制化，映射到一个二进制值空间（汉明空间）。

$$H(X) = sgn(P(X))$$

其中$sgn : \mathbb{R} \rightarrow \{-1,1\}^m$为元素级函数，以0为阈值将投射数据的每个维度二进制化。

但是，二进制化会带来邻居损失，可由如下公式定义此损失：

$$\mathcal{L} = \left \| S-V \right \|^2_F$$

其中，$\left \| \cdot \right \|_F$为Frobenius范数（定义为矩阵各项元素平方的总和开根），$S, V \in \mathbb{R}^{n\times n}$分别为原始数据点$X$和二进制编码$B$的邻居关系矩阵。具体而言，$S$中第$i$行第$j$列元素就表示$x_i$和$x_j$的邻居关系。当然，巨大的$\mathcal{L}$会极度下降二进制编码在近似最近邻居搜索中的性能。

### PTH大致框架

本文提出一种名为**后调哈希**（Post Tune Hash, PTH）的新颖哈希模型，加入了一个后调阶段$R: \{-1, 1\}^m \rightarrow \{-1, 1\}^m$来改良二进化后的编码。这样，需要优化的邻居错误就变为：

$$PTH(X) = R(H(X))$$

需要说明的是，很多方法都可以被用于生成$H(X)$，因此后调过程可被用于广泛的哈希方法，帮助这些方法改良二进制编码并提升性能。

为了最小化邻居错误，本文需要将其转变为一个计算可解的问题。之前提到的$S$中的每个元素代表原始数据点的邻居关系，这里通过欧几里得距离$d(x_i, x_j)$来判断邻居关系：

$$S_{ij} = \begin{cases}
 1 & \text{ if } d(x_i, x_j) < \epsilon \\
 -1 & \text{ otherwise }
\end{cases}$$

其中，$\epsilon$为判别邻居关系的欧几里得距离阈值。而$V$中的每个元素表示二进制编码的邻居关系，定义为一个内积：

$$V_{ij} = (b_i \cdot b_j)/m$$

$V_{ij}$为一个范围为$\{-1, 1\}$的离散变量。$V_{ij}=1$代表$b_i$和$b_j$完全一样；$V_{ij}=-1$代表$b_i$和$b_j$完全不一样。现在，我们可以把邻居损失转化为如下公式：

$$\mathcal{L} = \left \| S - \frac{1}{m}B^TB\right \|^2_F$$

其中，$B \in \{-1, 1\}^{m\times n}$为$X$的二进制编码。

例子：

$$\begin{aligned}
\frac{1}{2}\begin{bmatrix}
 -1 & 1\\
  1 & -1
\end{bmatrix}^T\begin{bmatrix}
 -1 & 1\\
  1 & -1
\end{bmatrix} &= \frac{1}{2}\begin{bmatrix}
 -1 & 1\\
  1 & -1
\end{bmatrix}\begin{bmatrix}
 -1 & 1\\
  1 & -1
\end{bmatrix}\\
&= \frac{1}{2}\begin{bmatrix}
 2 & -2\\
  -2 & 2
\end{bmatrix}\\
&= \begin{bmatrix}
 1 & -1\\
  -1 & 1
\end{bmatrix}
\end{aligned}$$

---

PTH将通过一个二进制矩阵$U$作为后调矩阵来改良二进制编码$Z=H(X) \in \{-1, 1\}^{m\times n}$，从而最小化$\mathcal{L}$。后调会重构破坏的邻居结构，并因此极大提升索引能力。通过使用$U$，后调阶段的结果很自然也会是二进制的，优化公式如下：

$$\begin{aligned}
&\min \mathcal{Q}(U) = \left \| S - \frac{1}{m}(U \circ Z)^T(U \circ Z)\right \|_F^2\\
&subject\ \ \ to \ \ \ \ \ \ u_{ij} \in \{-1, 1\}
\end{aligned}$$

其中，$\circ$为阿达玛乘积（元素级乘积），$u_{ij}$代表$U$的第$i$行第$j$列的元素。准确地说，$u_{ij}$表示$Z$矩阵的第$i$行第$j$列的元素是否应该翻转（$1$与$-1$互换）来最小化邻居错误。最终的二进制编码就被定义为$B = U \circ Z$。

阿达玛乘积：

$$\begin{bmatrix}
 a_{11} & a_{12} & a_{13}\\
 a_{21} & a_{22} & a_{23}\\
 a_{31} & a_{32} & a_{33}
\end{bmatrix} \circ \begin{bmatrix}
 b_{11} & b_{12} & b_{13}\\
 b_{21} & b_{22} & b_{23}\\
 b_{31} & b_{32} & b_{33}
\end{bmatrix} = \begin{bmatrix}
 a_{11}b_{11} & a_{12}b_{12} & a_{13}b_{13}\\
 a_{21}b_{21} & a_{22}b_{22} & a_{23}b_{23}\\
 a_{31}b_{31} & a_{32}b_{32} & a_{33}b_{33}
\end{bmatrix}$$

---

### PTH优化算法

尽管二进制编码的长度为1（$m=1$），为上述优化公式找到一个最优的$U$也是NP难问题，朴素的解决时间复杂度为$O(2^{m \times n})$。但本文可以通过一个实际算法来获得一个局部最优解。

本文注意到：目标函数的所有二次项都是常数，最小化$\mathcal{Q}(U)$就是最小化线性项之和。

因此，本文就从元素级的角度进行优化。首先，对每个$U$的元素计算线性项。令$\gamma = 1/m$，目标函数可以被转化为：

$$\min_U \sum_{ij}\left [ S_{ij} - \gamma \left (\sum_{k=1}^mu_{ki}z_{ki}u_{kj}z_{kj}\right )\right ]^2$$

将$U$中第$p$行看成变量，其他看为常数，线性项为：

$$-2 \sum_{ij} u_{pi}z_{pi}u_{pj}z_{pj}\left [\gamma S_{ij} - \gamma^2 \left(\sum^m_{\begin{aligned}k=1\\k\ne p\end{aligned}}u_{ki} z_{ki} u_{kj} z_{kj}\right)\right ]$$

令$\bar{z}_p$为$Z$第$p$行向量，$Q_{ij}$为矩阵$Q=\bar{z}_p \bar{z}_p^T$第$i$行第$j$列的元素。令$(U \circ Z)_{\setminus p}$为$U\circ Z$去除第$p$行的矩阵。令$O_{ij}$为矩阵$O = [(U \circ Z)_{\setminus p}]^T \cdot [(U \circ Z)_{\setminus p}]$的第$i$行第$j$列元素。因此上述式子可以转化为：

$$-2\gamma \sum_{ij}u_{pi}Q_{ij}(S_{ij} - \gamma O_{ij})u_{pj}$$

令$c_{ij}$为对称矩阵$C = Q \circ (S - \gamma O)$的第$i$行第$j$列元素。将$u_{pq}$作为变量，其他作为常数，线性项为：

$$-4\gamma \left (\sum_{\begin{aligned}k=1\\k\ne p\end{aligned}} u_{pk} c_{qk} \right )u_{pq}$$

（利用对称性，同时$i=j$时$c_{ij}=0$）

对于元素$u_{pq}$，最小化$\mathcal{Q}(U)$就是最小化上述式子。线性参数$-4\gamma (\sum_{k\ne q}u_{pk} c_{qk})$可被当做$u_{pq}$的权重。因此，如果线性系数$\le 0$，$u_{pq}=1$；否则为$-1$。

可以看出，决定一个$u_{pq}$需要其他$U$中的元素参与，这样会存在干扰。因此本文采用一个贪心算法来求解：将$U$所有元素都设为$1$，并每次迭代会更新最大线性系数的元素，直到最后的增益很小。基本的后调算法就是这样，下面是一些优化：

**更新策略**：本文将更新策略改为“如果系数大于一个固定阈值$\eta$就更新系数”。这样就可以按序处理元素，极大减少算法复杂度。实验中，$\eta$被设为所有系数的均值。另外，本文为更新一行的$U$提供相同的矩阵$C$，因为$C$被$U$所有行所决定，不会对一行数据的改变敏感。

**削减策略**：本文只会微调那些映射结果接近于0，或者小于阈值$\delta$的元素。因为这些元素有更高概率会被二进制为错误的值。这个策略会减少后调的复杂型，并有效防止过拟合。实验中，$\delta$被设为投射结果的绝对值均值。

![Fig2](./fig/post%20tuning%20hash.png)

整个算法可以迭代$K$次，通过查看目标函数的值的趋势，本文将$K$设为$5$。

![Fig3](./fig/post%20tuning%20hash%20iteration.png)

### 样本外后调

目前，后调算法可以对已知数据$X$进行二进制编码改良。下一个关键问题自然就是泛化后调算法，使其能处理新数据（$q \notin X$）。当然，样本外数据必须是和$X$保持一致的。因此，本文将$X$作为骨架点（$X$决定了后调的重新编码规则）。那么整个后调包括两步：后调骨架点$X$和后调与$X$一致的样本外点。

本文后调任何骨架点$x_i \in X$的目的是减少$x_i$编码与其他$X$中点的编码之间的不正确关系。考虑到一致性，对样本外的点进行微调也应该减小$q$和$X$之间的不正确关系。令$z^q$为$q$的原始二进制编码，样本外后调的目标函数可被定义为：

$$\begin{aligned}&\min \mathcal{R}(u^q) = \left \| S^q - \frac{1}{m}(u^q \circ z^q)^T B \right \|^2_F\\
&subject\ \ \ to \ \ \ \ \ \ u^q \in \{-1, 1\}
\end{aligned}$$

其中，$S^q$为$q$和$X$真实的邻居关系（欧几里得距离阈值）。$B$为$X$的后调编码。$u^q$为需要学习的后调向量。$u^q \circ z^q$为$q$的后调编码。那么上述后调算法少加改动就可以解决这个问题。通过以骨架点为基准，样本外点的邻居结构就可以被重建。由于后调是一个二进制后的独立学习过程，因此$X$可以与哈希训练集不同。实验中，本文在哈希训练集中选择了一小部分点作为$X$。

![Fig4](./fig/post%20tuning%20hash%20out-sample.png)

### 复杂度

**空间复杂度**：PTH的空间复杂度为$O(n \times m) + O(k \times l)$，其中$n$为数据集的大小、$m$为二进制编码的维度、$k$是骨架点的数量、$l$是原始特征的空间。第一项是用来存储二进制编码的（所有哈希方法都会使用）；第二项为PTH的额外花销。由于骨架点较少，因此PTH的额外空间花销不大。

**时间复杂度**：给定一个请求，需要花费$O(k\times d + k \times m \times m)$来生成二进制编码。$d$为原始特征的维度。$O(k \times m \times m)$是为了计算二进制编码的点积（矩阵$O$），可以通过比特级操作来加速。另外，PTH需要$O(k\times k \times d)$的时间为后调骨架点进行离线训练。

## 实验结果

5个图片benchmark实验结果现实本文的PTH模型在平均精确度均值上相比之前的SOTA方法提升了13-58%。

![Fig5](./fig/post%20tuning%20hash%20CIFAR.png)

![Fig6](./fig/post%20tuning%20hash%20NUSWIDE.png)

![Fig7](./fig/post%20tuning%20hash%20USPS.png)

![Fig8](./fig/post%20tuning%20hash%20MNIST.png)

![Fig9](./fig/post%20tuning%20hash%20ANN-GIST1M.png)

![Fig10](./fig/post%20tuning%20hash%20compare%20PQ.png)

骨架点的数量太多对效果的提升没有太大的帮助。另外，随着数据集大小的增加，骨架点的数量保持在100左右就可以有很好的效果。

![Fig11](./fig/post%20tuning%20hash%20skeleton%20points.png)

![Fig12](./fig/post%20tuning%20hash%20dateset%20size.png)

PTH所用的训练计算时间较长，而单步请求时间并没有太多增加。

![Fig13](./fig/post%20tuning%20hash%20computation%20time.png)

## 个人感想

论文特色：

- 后处理的思想，打破预处理的思维枷锁。
- 将一个NP难问题转化为一个可求局部最优解的方法。
- 算法的各方面设定都具备合理性，贴近实际使用。
- 灵活使用离散值-1、1的性质。

创新方向：

- 持续哈希：数据按序送过来，如何逐渐构建数据间的邻居关系？
- 视频哈希：对视频每一帧进行哈希，看看能不能把帧的顺序排列成功。
- 哈希二进制编码与深度学习编码器结合，提高可解释性。
